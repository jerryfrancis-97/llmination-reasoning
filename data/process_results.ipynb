{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /Users/serenapei/llmination-reasoning/results/math_results_20250513_121754.pkl...\n",
      "\n",
      "===== BASIC INFORMATION =====\n",
      "Total records: 50\n",
      "Memory usage: 0.05 MB\n",
      "Columns: prompt_id, prompt_text, api, model, answer, reasoning_type, confidence, response_time, timestamp, problem_type, subject, level, original_answer\n",
      "\n",
      "===== MODEL DISTRIBUTION =====\n",
      "api   model         \n",
      "groq  llama3-8b-8192    50\n",
      "dtype: int64\n",
      "\n",
      "===== REASONING TYPE DISTRIBUTION =====\n",
      "reasoning_type\n",
      "Reasoning        30\n",
      "Uncertain        13\n",
      "Hallucination     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Percentage:\n",
      "reasoning_type\n",
      "Reasoning        60.0\n",
      "Uncertain        26.0\n",
      "Hallucination    14.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "===== CONFIDENCE STATISTICS =====\n",
      "Mean confidence: 97.70%\n",
      "Median confidence: 100.00%\n",
      "Confidence by reasoning type:\n",
      "reasoning_type\n",
      "Hallucination        100.0\n",
      "Reasoning             98.4\n",
      "Uncertain        94.846154\n",
      "Name: confidence, dtype: object\n",
      "\n",
      "===== RESPONSE TIME STATISTICS (seconds) =====\n",
      "Mean response time: 1.27s\n",
      "Median response time: 0.21s\n",
      "Response time by model:\n",
      "api   model         \n",
      "groq  llama3-8b-8192    1.26706\n",
      "Name: response_time, dtype: float64\n",
      "\n",
      "===== SAMPLE RESPONSES =====\n",
      "\n",
      "Sample response from groq/llama3-8b-8192:\n",
      "Problem: The proper divisors of $1.2 \\times 10^{12}$ are numerous.  A proper divisor of an integer $N$ is a positive divisor of $N$ that is less than $N$. What...\n",
      "Answer: ...\n",
      "Reasoning: Uncertain\n",
      "Confidence: 100%\n",
      "--------------------------------------------------\n",
      "\n",
      "Summary saved to /Users/serenapei/llmination-reasoning/results/math_results_20250513_121754_summary.csv\n",
      "\n",
      "Inspection complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Path to your results file\n",
    "results_file = \"/Users/serenapei/llmination-reasoning/results/math_results_20250513_121754.pkl\"\n",
    "\n",
    "def inspect_results(file_path):\n",
    "    \"\"\"Load and inspect the results data\"\"\"\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load the pickle file\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    try:\n",
    "        df = pd.read_pickle(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\n===== BASIC INFORMATION =====\")\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / (1024*1024):.2f} MB\")\n",
    "    print(f\"Columns: {', '.join(df.columns)}\")\n",
    "    \n",
    "    # Model distribution\n",
    "    print(f\"\\n===== MODEL DISTRIBUTION =====\")\n",
    "    model_counts = df.groupby(['api', 'model']).size()\n",
    "    print(model_counts)\n",
    "    \n",
    "    # Reasoning type distribution \n",
    "    print(f\"\\n===== REASONING TYPE DISTRIBUTION =====\")\n",
    "    reasoning_counts = df['reasoning_type'].value_counts()\n",
    "    print(reasoning_counts)\n",
    "    print(\"\\nPercentage:\")\n",
    "    print(reasoning_counts / len(df) * 100)\n",
    "    \n",
    "    # Confidence stats\n",
    "    if 'confidence' in df.columns and df['confidence'].notna().any():\n",
    "        print(f\"\\n===== CONFIDENCE STATISTICS =====\")\n",
    "        print(f\"Mean confidence: {df['confidence'].mean():.2f}%\")\n",
    "        print(f\"Median confidence: {df['confidence'].median():.2f}%\")\n",
    "        print(f\"Confidence by reasoning type:\")\n",
    "        print(df.groupby('reasoning_type')['confidence'].mean().sort_values(ascending=False))\n",
    "    \n",
    "    # Response time\n",
    "    print(f\"\\n===== RESPONSE TIME STATISTICS (seconds) =====\")\n",
    "    print(f\"Mean response time: {df['response_time'].mean():.2f}s\")\n",
    "    print(f\"Median response time: {df['response_time'].median():.2f}s\")\n",
    "    print(f\"Response time by model:\")\n",
    "    print(df.groupby(['api', 'model'])['response_time'].mean().sort_values())\n",
    "    \n",
    "    # Accuracy analysis (if applicable)\n",
    "    if 'correct' in df.columns:\n",
    "        print(f\"\\n===== ACCURACY ANALYSIS =====\")\n",
    "        overall_acc = df['correct'].mean() * 100\n",
    "        print(f\"Overall accuracy: {overall_acc:.2f}%\")\n",
    "        \n",
    "        print(\"\\nAccuracy by model:\")\n",
    "        model_acc = df.groupby(['api', 'model'])['correct'].agg(['mean', 'count'])\n",
    "        model_acc['mean'] = model_acc['mean'] * 100\n",
    "        print(model_acc.sort_values('mean', ascending=False))\n",
    "        \n",
    "        if 'problem_type' in df.columns:\n",
    "            print(\"\\nAccuracy by problem type:\")\n",
    "            type_acc = df.groupby('problem_type')['correct'].agg(['mean', 'count'])\n",
    "            type_acc['mean'] = type_acc['mean'] * 100\n",
    "            print(type_acc.sort_values('mean', ascending=False))\n",
    "        \n",
    "        print(\"\\nAccuracy by reasoning type:\")\n",
    "        reason_acc = df.groupby('reasoning_type')['correct'].agg(['mean', 'count'])\n",
    "        reason_acc['mean'] = reason_acc['mean'] * 100\n",
    "        print(reason_acc.sort_values('mean', ascending=False))\n",
    "        \n",
    "        # Check relationship between confidence and accuracy\n",
    "        if 'confidence' in df.columns and df['confidence'].notna().any():\n",
    "            print(\"\\nCorrelation between confidence and accuracy:\")\n",
    "            correlation = df['confidence'].corr(df['correct'])\n",
    "            print(f\"Correlation coefficient: {correlation:.4f}\")\n",
    "            \n",
    "            # Group by confidence bands\n",
    "            df['confidence_band'] = pd.cut(df['confidence'], \n",
    "                                          bins=[0, 25, 50, 75, 90, 100], \n",
    "                                          labels=['0-25%', '26-50%', '51-75%', '76-90%', '91-100%'])\n",
    "            conf_acc = df.groupby('confidence_band')['correct'].agg(['mean', 'count'])\n",
    "            conf_acc['mean'] = conf_acc['mean'] * 100\n",
    "            print(\"\\nAccuracy by confidence band:\")\n",
    "            print(conf_acc)\n",
    "    \n",
    "    # Sample responses\n",
    "    print(f\"\\n===== SAMPLE RESPONSES =====\")\n",
    "    # Show a few example responses from different models\n",
    "    for (api, model), group in df.groupby(['api', 'model']):\n",
    "        if len(group) > 0:\n",
    "            print(f\"\\nSample response from {api}/{model}:\")\n",
    "            sample = group.iloc[0]\n",
    "            print(f\"Problem: {sample['prompt_text'][:150]}...\")\n",
    "            print(f\"Answer: {sample['answer'][:150]}...\")\n",
    "            print(f\"Reasoning: {sample['reasoning_type']}\")\n",
    "            if 'confidence' in df.columns and pd.notna(sample['confidence']):\n",
    "                print(f\"Confidence: {sample['confidence']}%\")\n",
    "            if 'correct' in df.columns:\n",
    "                print(f\"Correct: {sample['correct']}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Execute the inspection\n",
    "df = inspect_results(results_file)\n",
    "\n",
    "# Optionally save summarized results to a CSV for easier viewing\n",
    "if df is not None:\n",
    "    summary_file = os.path.splitext(results_file)[0] + \"_summary.csv\"\n",
    "    df_summary = df[['api', 'model', 'prompt_id', 'problem_type', 'reasoning_type', \n",
    "                     'confidence', 'response_time']]\n",
    "    if 'correct' in df.columns:\n",
    "        df_summary = df_summary.join(df[['correct']])\n",
    "    df_summary.to_csv(summary_file, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_file}\")\n",
    "\n",
    "print(\"\\nInspection complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_problem_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
